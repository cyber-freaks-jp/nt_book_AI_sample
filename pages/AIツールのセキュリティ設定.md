# AIツールのセキュリティ設定

AIツールの設定を適切に行うことで、情報漏洩や破壊的な操作などのセキュリティリスクを大幅に削減できます。

---

## AIのセキュリティリスクを防ぐ3つの設定

### 設定1：学習データとして使用されない設定

AIに渡した情報が、AIの学習データとして使われないようにする設定です。

**なぜ重要か（情報漏洩リスクの防止）**：
- AIに渡した情報が学習データとして使われると、インターネット上で情報公開されてしまう可能性があります
- 機密情報やAPIキーが漏洩するリスクがあります

**Cursorでの設定例**：

Cursorには、送信したコードを学習データとして使用させない設定があります。

具体的には、Cursorの設定画面で「Privacy Mode」をオンにします。

（具体的な設定手順は、Cursorのバージョンによって異なるため、公式ドキュメントを参照してください）

**他のツールでも同様の設定がありますので、ツールに応じて設定をググりましょう**


**⚠️ 重要：設定だけでは完全ではない**

いくら設定しても、**どこまで信用できるかはわかりません**：
- AIプロバイダーが意図せず情報漏洩するリスクもあります
- セキュリティ侵害や内部犯行の可能性もゼロではありません

**結論：そもそも機密情報をAIに渡さないことが一番大事です。**

設定はリスク軽減のために行うべきですが、過信は禁物です。最も確実な対策は、**機密情報をAIに渡さないこと**です。

---

### 設定2：機密ファイルの除外設定

各AIツールには、**特定のファイルをAIの読み取り対象から除外する設定**があります。

わかりやすい例でいうと、gitの場合は.gitignoreで、git管理から除外しますよね。
cursorの場合は、.cursorignoreで、AIの処理対象外にすることができます。

`.env`ファイルやAPIキーなど、機密情報を含むファイルは、このような仕組みを使ってAIが読み込めないように設定しましょう


**なぜ重要か（情報漏洩リスクの防止）**：
- `.env`ファイルやAPIキーなどの機密情報をAIに読み取らせると、情報漏洩のリスクがあります
- いくら学習しない設定をしても、そもそもAIに機密情報を渡すべきではありません

**どうやって除外するのか**：
**Cursorの例：`.cursorignore`ファイル**

`.cursorignore`とは、Gitの`.gitignore`ファイルと似た役割を果たす設定ファイルです。このファイルに記述されたルールに従って、CursorのAIが読み込まないようにするファイルを定義できます。

プロジェクトのルートディレクトリに`.cursorignore`ファイルを作成し、除外したいファイルを記述します：

```
# .cursorignore の例
.env
.env.*
*.key
*.pem
id_rsa
*.p12
credentials.json
config/secrets.yml
config/database.yml
```

この設定により、Cursorは上記のファイルをAIに送信しません。

**他のツールでも同様の仕組みがあります**：
- Claude Code：`.claudeignore`で除外
- その他のツール：各ツールのドキュメントを参照

**除外すべきファイルの例**：
- 環境変数ファイル（`.env`、`.env.local`など）
- 秘密鍵（`.pem`、`.key`、`id_rsa`など）
- 認証情報（`credentials.json`、`secrets.yml`など）
- データベース設定（`database.yml`など）


**⚠️ 重要：設定だけでは完全ではない**

繰り返しの警告になりますが、いくら設定しても、**どこまで信用できるかはわかりません**：

**結論：そもそも機密情報をAIの目の届くところに置くのは極力避けましょう。**

設定はリスク軽減のために行うべきですが、過信は禁物です。最も確実な対策は、**機密情報をAIの目の届く範囲に置かないこと**です。

一番理想なのは、Dockerなど隔離されたサンドボックス環境にAIを閉じ込め、その中には機密データを置かないことです


### 設定3：操作権限の設定

AIが実行できる操作（ファイル削除、外部API呼び出しなど）を制限する設定です。

**なぜ重要か（破壊的な操作リスクの防止）**：
- AIがファイル削除、データベース操作、外部API実行などを実行できてしまうと、データ破壊や環境破壊されてしまうリスクがあります
- 必要最小限の権限のみを与えることで、リスクを最小化できます

**Claude Codeでの設定例：`.claude/settings.local.json`ファイル**

`.claude/settings.local.json`とは、Claude Codeに対してAIの動作権限を設定するためのファイルです。

```json
{
  "dangerouslyDisablePermissions": false,
  "allowedCommands": [
    "grep",
    "find"
  ],
  "blockedCommands": [
    "rm",
    "sudo",
  ],
  "allowedPaths": [
    "/workspace",
    "/tmp"
  ],
  "blockedPaths": [
    "~/.ssh"
  ]
}
```

この設定により、Claude CodeのAIは許可されたコマンドとパスのみにアクセスできます。

**他のツールでも同様の仕組みがあります**：
- Cursor：`.cursorrules`でルール設定
- その他のツール：各ツールのドキュメントを参照

**禁止すべき操作の例**：
- ファイルの削除（`rm -rf`など）
- `sudo`コマンドの使用


**⚠️ 重要：この設定よりもサンドボックス環境の用意をする方が大事**

別の章で解説しましたが、この設定を入れる以前に、Dockerなどの隔離されたサンドボックス環境内でのみAIを動かすルールにしてしまえば、そもそもこの設定は必要ありません。

むしろ、この設定を入れると、操作のたびに人間に許可を求めてくるので生産性が落ちます。

最もおすすめなのは、サンドボックス環境内で、むしろ「全操作を許可設定」することです。
これにより、安全かつ、AIが完全自動実行できて生産性の高い環境が手に入ります

---

## まとめ

### AIのセキュリティリスクを防ぐ3つの設定

1. **学習データとして使用されない設定をする**（情報漏洩リスクの防止）
2. **機密ファイルを除外設定する**（情報漏洩リスクの防止）
3. **操作権限を適切に設定する or サンドボックス環境を用意する**（破壊的な操作リスクの防止）
